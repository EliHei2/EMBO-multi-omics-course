---
title: "Horizontal Data Integration"
author: "Constantin Ahlmann-Eltze"
date: today
format: 
  html:
    code-fold: false
    code-tools: true
    embed-resources: true
    highlight-style: github
    toc: true 
    code-line-numbers: false 
bibliography: references.bib
params:
  skip_execution: false
  skip_slow_chunks: true
  skip_answers: true
---

![Figure 1 from *Computational principles and challenges in single-cell data integration* by @argelaguet2021. Horizontal data integration is concerned with relating cells measured in different conditions or batches where we have the same features (i.e., genes) for the cells.](images/horizontal_integration_screenshot.png)

To start, we will load the `tidyverse` and `SingleCellExperiment` packages:

```{r}
#| label: load_packages
#| output: false
#| eval: !expr (! params$skip_execution)
library(tidyverse)
library(SingleCellExperiment)
```

# Example data

We will use a popular dataset by @kang2018 for this tutorial. The dataset measured the effect of interferon-$\beta$ stimulation on blood cells from eight patients. The [`muscData`](https://bioconductor.org/packages/muscData/) package provides an easy way to access the data as a [`SingleCellExperiment`](https://bioconductor.org/books/release/OSCA.intro/the-singlecellexperiment-class.html). 
In case downloading the data with the `muscData` package fails, you can also directly download the file from <http://oc.embl.de/index.php/s/tpbYcH5P9NfXeM5> and load it into R using `sce <- readRDS("~/Downloads/PATH_TO_THE_FILE")`.

```{r}
#| label: load_kang_data
#| eval: !expr (! isTRUE(params$skip_execution))
sce <- muscData::Kang18_8vs8()
sce
```


::: {.callout-note collapse="true"}
## Challenge: How many genes and cells are in the data? How do you find the metadata about each cell / gene?

You can find the number of genes and cells when printing the summary of the `sce`. Alternatively, you can use `nrow(sce)`, `ncol(sce)`, or `dim(sce)` to gt these values.

In a `SingleCellExperiment` object, the meta information about each cell using `colData(sce)` and for each gene with `rowData(sce)`. 

Follow-up question: why does the documentation for `colData` (run `?colData` in the R console) talk about `SummarizedExperiment` objects instead of `SingleCellExperiment`?
:::

We log-transform the data to account for the heteroskedasticity of the counts, perform PCA to reduce the dimensions, and run UMAP for visualization. For the preprocessing, we will use the [`scater`](https://bioconductor.org/packages/scater/) package, which adds a new assay called `"logcounts"` and two `reducedDims(sce)` called `"PCA"` and `"UMAP"` to the `SummarizedExperiment` object. Equivalent preprocessing functions also exist in Seurat or scanpy.

```{r}
#| label: kang_preprocess
#| eval: !expr (! params$skip_execution)
sce <- scater::logNormCounts(sce)
hvg <- order(MatrixGenerics::rowVars(logcounts(sce)), decreasing = TRUE)
sce <- sce[hvg[1:500], ]
sce <- scater::runPCA(sce, ncomponents = 50)
sce <- scater::runUMAP(sce, dimred = "PCA")
```

::: {.callout-note collapse="true"}
## Challenge: How would you do a sctransform-like transformation (i.e., Pearson residuals) without using Seurat?

The [transformGamPoi](https://bioconductor.org/packages/release/bioc/vignettes/transformGamPoi/inst/doc/transformGamPoi.html) package from Bioconductor provides a `residual_transform` function.

```{r}
#| label: sctransform_alternative
#| eval: !expr (! params$skip_execution && ! params$skip_slow_chunks)
assay(sce, "pearson_residuals") <- transformGamPoi::residual_transform(sce, residual = "pearson", on_disk = FALSE)
```
:::

::: {.callout-note collapse="true"}
## Question: What is the problem with using very few components for PCA? Is there also a problem with using too many?

Too few dimensions for PCA mean that it cannot capture enough of the relevant variation in the data. This leads to a loss of subtle differences between cell states.

Too many dimensions for PCA can also pose a problem. PCA smoothes out the Poisson noise uncorrelated that is orthogonal to the biological signal. If too many PCA components are included, the additional dimensions capture the noise that can obscure relevant differences. For more details see Fig. 2d in @ahlmann-eltze2023.
:::

::: {.callout-note collapse="true"}
## Challenge: How can you use tSNE instead of UMAP? What are alternative visualization methods?

The scater package also provides a `runTSNE` function

```{r}
#| label: run_tsne
#| eval: !expr (! params$skip_execution && ! params$skip_slow_chunks)
sce <- scater::runTSNE(sce, dimred = "PCA")
```

On the one hand, tSNE and UMAP are defacto standards for visualizing the cell states in single cell data analysis. On the other hand, they are often criticized for failing to represent global structure and distorting differences in the data (just look at twitter). A number of alternatives have been suggested:

- Force directed layout of the $k$ nearest neighbor (kNN) graph ([igraph](https://igraph.org/r/doc/layout_with_fr.html)),
- [PHATE](https://github.com/KrishnaswamyLab/PHATE)
- [IVIS](https://bering-ivis.readthedocs.io/en/latest/index.html)
- Unification of tSNE and UMAP using contrastive learning ([BÃ¶hm, 2023](https://arxiv.org/pdf/2210.09879.pdf))
:::



To visualize the data, we use `ggplot2`. This is often more verbose than calling an existing function (e.g., `scater::plotReducedDim(sce, "UMAP", color_by = "stim")`) but has the advantage that the plots are easier to customize.

In the UMAP, the cells separate by treatment status (`"stim"`) and cell type (`"cell"`). The goal of this tutorial is to understand how different models adjust for the known treatment status and integrate the data into a shared embedding.

```{r}
#| label: fig-kang-umap
#| fig-cap: UMAP of log transformed counts
#| eval: !expr (! params$skip_execution)
as_tibble(colData(sce)) |>
  mutate(umap = reducedDim(sce, "UMAP")) |>
  ggplot(aes(x = umap[,1], y = umap[,2])) +
    geom_point(aes(color = stim), size = 0.3) +
    coord_fixed()
```

This dataset already comes with cell type annotations. The cell type labels are helpful for interpreting the results; however, for the purpose of this tutorial, we will not need them and will ignore them from now on.

```{r}
#| label: fig-kang-umap2
#| fig-cap: Cell type labels
#| eval: !expr (! params$skip_execution)
as_tibble(colData(sce)) |>
  mutate(umap = reducedDim(sce, "UMAP")) |>
  ggplot(aes(x = umap[,1], y = umap[,2])) +
    geom_point(aes(color = cell), size = 0.3) +
    ggrepel::geom_text_repel(data = \(dat) dat |> summarize(umap = matrix(colMedians(umap), nrow = 1), .by = c(stim, cell)),
              aes(label = cell)) +
    coord_fixed()
```


::: {.callout-note collapse="true"}
## Challenge: Do the preprocessing with Seurat (click to see results)

The following code is based on Seurat's [*Guided Clustering Tutorial*](https://satijalab.org/seurat/archive/v3.0/pbmc3k_tutorial.html).

```{r}
#| label: seurat-preprocessing
#| eval: !expr (! params$skip_execution && ! params$skip_answers)
# For more information about the conversion see `?as.Seurat.CellDataSet`
seur_obj <- Seurat::as.Seurat(muscData::Kang18_8vs8(), data = NULL)
seur_obj <- Seurat::NormalizeData(seur_obj, normalization.method = "LogNormalize", scale.factor = 10000)
seur_obj <- Seurat::FindVariableFeatures(seur_obj, selection.method = "vst", nfeatures = 500)
# Subset to highly variable genes for memory efficiency
seur_obj <- seur_obj[Seurat::VariableFeatures(object = seur_obj),]
seur_obj <- Seurat::ScaleData(seur_obj)
seur_obj <- Seurat::RunPCA(seur_obj, verbose = FALSE)
seur_obj <-Seurat::RunUMAP(seur_obj, dims = 1:10)
```

```{r}
#| label: fig-seurat-plot
#| fig-cap: "UMAP plot after preprocessing with Seurat"
#| eval: !expr (! params$skip_execution && !params$skip_answers)
Seurat::DimPlot(seur_obj, reduction = "umap", group.by = "stim")
```
:::

# Data integration

@fig-kang-umap shows that the data separates by the treatment status. For many downstream analyses, it would be good to know how the cells from the stimulated condition are related to the cells from the control condition. For example for cell type assignment, we might want to annotate both conditions together and ignore the effect of the treatment. This process is called **integration**.

The goal is a low-dimensional embedding of the cells where the treatment status does not affect the embedding and all residual variance comes from different cell states. @fig-integrated_picture shows a successfully integrated example.

![UMAP of a successfully integrated dataset.](images/integrated_data_picture.png){#fig-integrated_picture}

There are many methods for single-cell data integration, and @luecken2022 benchmarked several approaches. Here, I will present four integration methods that are easy to use from R and cover a useful set of features:

-   Manual projection
-   Automated integration
    -   Harmony
    -   MNN
-   Invertible integration
    -   LEMUR
    
::: {.callout-note collapse="true"}
## Question: In this tutorial we will just look at plots to assess integration success. Why is that suboptimal? How can we do better?

A 2D embedding like UMAP or tSNE gives us a first impression if cells from different conditions are mixed, but the results are not quantitative which means we cannot directly compare the outcome.

One simple metric to measure integration success is to see how mixed the conditions are. For example we can count for each cell how many of the nearest neighbors come from the same condition and how many come from the other conditions. For more information see @luecken2022.

Follow-up challenge: Write a function to calculate the mixing metric.

Follow-up questions: Can you write an integration function, that scores perfectly on the integration metric? Hint: it can be biologically completely useless. What else would you need to measure to protect against such a pathological example.

:::


## Manual Projection

![Schematic picture of data from two conditions. The data from the treated condition is projected onto the subspace spanned by the control condition.](images/Subspace_illustration_both_condition_projection.png){#fig-ctrl-proj width="40%"}

In transcriptomic data analysis, each cell is characterized by its gene expression values. In our case, these are the 500 most variable genes. Accordingly, each cell is a point in a 500-dimensional _gene expression_ space. Principal component analysis (PCA) finds a $P$-dimensional space with minimal distance to each cell. 

To make these concepts more tangible, I created a cartoon shown in Figure @fig-ctrl-proj. The orange blob represents all cells from the control condition in a 3-dimensional gene expression space. The grey rectangle $R$ is a lower-dimensional subspace covering the shape of the orange blob. The shape of the blue blob (i.e., the treated cells) resembles the orange blob but is slightly offset. To integrate both conditions, we can project each point from the blue blob onto the subspace covering the orange blob.

We can implement this procedure in a few lines of R code:

1. We create a matrix for the cells from the control and treated conditions,
2. we fit PCA on the control cells,
3. we center the data, and finally
4. we project the cells from both conditions onto the subspace of the control condition

```{r}
#| label: manual-proj
#| eval: !expr (! params$skip_execution)
# Each column from the matrix is the coordinate of a cell in the 500-dimensional space
ctrl_mat <- as.matrix(logcounts(sce)[,sce$stim == "ctrl"])
stim_mat <- as.matrix(logcounts(sce)[,sce$stim == "stim"])

ctrl_centers <- rowMeans(ctrl_mat)
stim_centers <- rowMeans(stim_mat)

# `prcomp` is R's name for PCA and IRLBA is an algorithm to calculate it.
ctrl_pca <- irlba::prcomp_irlba(t(ctrl_mat - ctrl_centers), n = 20)

# With a little bit of linear algebra, we project the points onto the subspace of the control cells
integrated_mat <- matrix(NA, nrow = 20, ncol = ncol(sce))
integrated_mat[,sce$stim == "ctrl"] <- t(ctrl_pca$rotation) %*% (ctrl_mat - ctrl_centers)
integrated_mat[,sce$stim == "stim"] <- t(ctrl_pca$rotation) %*% (stim_mat - stim_centers)
```

We check that our implementation works, by looking at the UMAP of the integrated data. 

```{r}
#| label: fig-manual-umap
#| fig-cap: UMAP of log transformed counts
#| collapse: true
#| eval: !expr (! params$skip_execution)

int_mat_umap <- uwot::umap(t(integrated_mat))

as_tibble(colData(sce)) |>
  mutate(umap = int_mat_umap) |>
  sample_frac() |>
  ggplot(aes(x = umap[,1], y = umap[,2])) +
    geom_point(aes(color = stim), size = 0.3) +
    coord_fixed()
```

The overlap is not perfect, but better than in @fig-kang-umap!

::: {.callout-note collapse="true"}
## Challenge: What happens if you project on the `"stim"` condition?

![Schematic picture of data from two conditions using the stimulated condition as reference.](images/Subspace_illustration_both_condition_projection2.png){width="40%"}

The projection is orthogonal onto the subspace, which means it matters which condition is chosen as reference.

```{r}
#| label: rev-manual-proj
#| eval: !expr (! params$skip_execution && ! params$skip_answers)
stim_pca <- irlba::prcomp_irlba(t(stim_mat - stim_centers), n = 20, center = FALSE)

integrated_mat2 <- matrix(NA, nrow = 20, ncol = ncol(sce))
integrated_mat2[,sce$stim == "ctrl"] <- t(stim_pca$rotation) %*% (ctrl_mat - ctrl_centers)
integrated_mat2[,sce$stim == "stim"] <- t(stim_pca$rotation) %*% (stim_mat - stim_centers)

int_mat_umap2 <- uwot::umap(t(integrated_mat2))

as_tibble(colData(sce)) |>
  mutate(umap = int_mat_umap2) |>
  sample_frac() |>
  ggplot(aes(x = umap[,1], y = umap[,2])) +
    geom_point(aes(color = stim), size = 0.3) +
    coord_fixed()
```

For this example, using the `"stim"` condition as the reference leads to a worse integration.
:::

::: {.callout-warning collapse="true"}
## Brain teaser: How can you make the manual projection approach work for any complex experimental designs?

The projection approach consists of three steps:

1.  Centering the data (e.g., `ctrl_mat - ctrl_centers`).
2.  Choosing a reference condition and calculating the subspace that approximates the data from the reference condition (`irlba::prcomp_irlba(t(stim_mat - stim_centers))$rotation`).
3.  Projecting the data from the other conditions onto that subspace (`t(ctrl_pca$rotation) %*% (stim_mat - stim_centers)`).

For arbitrary experimental designs, we can perform the centering with a linear model fit. The second step remains the same. And after calculating the centered matrix, the third step is also straight forward. Below are the outlines how such a general procedure would work.

```{r}
#| label: challenge-complex-manual-adjustment
#| eval: false
# A complex experimental design
lm_fit <- lm(t(logcounts(sce)) ~ condition + batch, data = colData(sce))
# The `residuals` function returns the coordinates minus the mean at the condition.
centered_mat <- t(residuals(lm_fit))
# Assuming that `is_reference_condition` contains a selection of the cells
ref_pca <- irlba::prcomp_irlba(centered_mat[,is_reference_condition], ...)
int_mat <- t(ref_pca$rotation) %*% centered_mat
```
:::

## Automatic integration

In the following, I show three approaches for automatically integrating the data. You don't need to run all three. Usually, you pick the method that you like best. Here, I provide the code for all three so that you can compare for yourself.

### Harmony

Harmony is one popular tool for data integration [@korsunsky2019]. Harmony is relatively fast and can handle more complex experimental designs than just a treatment/control setup. It is built around _maximum diversity clustering_ (@fig-harmony_schematic). Unlike classical clustering algorithms, maximum diversity clustering not just minimizes the distance of each data point to a cluster center but also maximizes the mixing of conditions assigned to each cluster.

![Schematic of Harmony. Screenshot from Fig. 1 of @korsunsky2019](images/harmony_schematic.png){#fig-harmony_schematic}

```{r}
#| label: harmony_integration
#| eval: !expr (! params$skip_execution)
# Warning: You need `packageVersion("harmony") >= "1.0.0"` for this to work.
harm_mat <- harmony::RunHarmony(reducedDim(sce, "PCA"), colData(sce), 
                                 vars_use = c("stim"))
harm_umap <- uwot::umap(harm_mat)

as_tibble(colData(sce)) |>
  mutate(umap = harm_umap) |>
  sample_frac() |>
  ggplot(aes(x = umap[,1], y = umap[,2])) +
    geom_point(aes(color = stim), size = 0.3) +
    coord_fixed()
```

::: {.callout-note}
## Challenge: How much do the results change if you change the default parameters in Harmony?
:::

### MNN

MNN is short for mutual nearest neighbors and was invented for integrating two conditions by identifying the cells that are mutually nearest neighbors [@fig-mnn_schematic]. The [`batchelor`](https://bioconductor.org/packages/batchelor/) package provides an efficient implementation that can also handle experimental designs with more than two conditions.

![Schematic of MNN Screenshot from Fig. 1 of @haghverdi2018](images/mnn_schematic.png){#fig-mnn_schematic width="40%"}

```{r}
#| label: mnn_integration
#| eval: !expr (! params$skip_execution)
mnn_sce <- batchelor::fastMNN(sce, batch = sce$stim, BSPARAM=BiocSingular::IrlbaParam())
mnn_umap <- uwot::umap(reducedDim(mnn_sce, "corrected"))

as_tibble(colData(sce)) |>
  mutate(umap = mnn_umap) |>
  sample_frac() |>
  ggplot(aes(x = umap[,1], y = umap[,2])) +
    geom_point(aes(color = stim), size = 0.3) +
    coord_fixed()
```

::: {.callout-note}
## Challenge: How much do the results change if you change the default parameters in MNN?
:::

Seurat's integration method is very similar to MNN (however, it calls the mutual nearest neighbors _integration anchors_). The main difference is that Seurat uses CCA instead of PCA for dimensionality reduction [@butler2018].

```{r}
#| label: fig-seurat_integration
#| fig-caption: "Seurat's anchor integration"
#| eval: !expr (! params$skip_execution && ! params$skip_slow_chunks)

# This code only works with Seurat v5!
seur_obj2 <- Seurat::as.Seurat(muscData::Kang18_8vs8(), data = NULL)
seur_obj2[["originalexp"]] <- split(seur_obj2[["originalexp"]], f = seur_obj2$stim)
seur_obj2 <- Seurat::NormalizeData(seur_obj2, normalization.method = "LogNormalize", scale.factor = 10000)
seur_obj2 <- Seurat::FindVariableFeatures(seur_obj2, selection.method = "vst", nfeatures = 500)
seur_obj2 <- Seurat::ScaleData(seur_obj2)
seur_obj2 <- Seurat::RunPCA(seur_obj2, verbose = FALSE)

seur_obj2 <- Seurat::IntegrateLayers(object = seur_obj2, method = Seurat::CCAIntegration, orig.reduction = "pca", new.reduction = "integrated.cca", verbose = FALSE)
seur_obj2 <- Seurat::RunUMAP(seur_obj2, dims = 1:30, reduction = "integrated.cca")
Seurat::DimPlot(seur_obj2, reduction = "umap", group.by = "stim")
```



# Invertible Integration

Tools like MNN and Harmony take a PCA embedding and remove the effects of the specified covariates. However, there is no way to go back from the integrated embedding to the original gene space. This means that we cannot ask the counterfactual what the expression of a cell from the control condition would have been, had it been treated.

A new tool called [LEMUR](https://bioconductor.org/packages/lemur/) provides this functionality by matching the subspace of each condition [@ahlmann-eltze2024]. @fig-subspace_matching illustrates the principle.

![Schematic picture of data from two conditions with the respective linear subspace.](images/Subspace_illustration_both_condition_with_arrow.png){#fig-subspace_matching width="40%"}

LEMUR takes as input a `SingleCellExperiment` object, the specification of the experimental design, and the number of latent dimensions. To refine the embedding, we will use the provided cell type annotations.
```{r}
#| label: fit-lemur-model
#| eval: !expr (! params$skip_execution)
fit <- lemur::lemur(sce, design = ~ stim, n_embedding = 30, verbose = FALSE)
fit <- lemur::align_by_grouping(fit, fit$colData$cell, verbose = FALSE)
```

::: {.callout-note}
## Challenge: How much do the results change if you change the default parameters in LEMUR?
:::

::: {.callout-note collapse="true"}
## Challenge: How to refine the embedding of LEMUR with an automated tool?

```{r}
#| label: align-lemur-model
#| eval: !expr (! params$skip_execution & ! params$skip_answers)
fit <- lemur::lemur(sce, design = ~ stim, n_embedding = 30, verbose = FALSE)
fit <- lemur::align_harmony(fit, verbose = FALSE)
```
:::

Making a UMAP plot of LEMUR's embedding shows that it successfully integrated the conditions (@fig-lemur_umap).

```{r}
#| label: fig-lemur_umap
#| fig-cap: "UMAP plot of LEMUR's invertible embedding."
#| eval: !expr (! params$skip_execution)
lemur_umap <- uwot::umap(t(fit$embedding))

as_tibble(colData(sce)) |>
  mutate(umap = lemur_umap) |>
  sample_frac() |>
  ggplot(aes(x = umap[,1], y = umap[,2])) +
    geom_point(aes(color = stim), size = 0.3) +
    coord_fixed()
```

The advantage of the invertible integration is that we can predict what a cell's expression from the control condition would have been, had it been stimulated and vice versa. Contrasting those predictions tells us how much the gene expression changes for that cell in any gene.

![Differential expression with an invertible integration](images/differential_expression.png){width="60%"}

We call LEMUR's `test_de` function to compare the expression values in the `"stim"` and `"ctrl"` conditions.
```{r}
#| label: lemur-calc-de
#| eval: !expr (! params$skip_execution)
fit <- lemur::test_de(fit, contrast = cond(stim = "stim") - cond(stim = "ctrl"))
```

We can now pick individual genes and plot the predicted log fold change for each cell to show how it varies as a function of the underlying gene expression values (@fig-lemur_plot_de-1).

```{r}
#| label: fig-lemur_plot_de
#| layout-ncol: 2
#| fig-cap: 
#|   - "Expression of _PLSCR1_ in control and stim condition"
#|   - "LEMUR's prediction of differential expression"
#| eval: !expr (! params$skip_execution)
as_tibble(colData(sce)) |>
  mutate(umap = lemur_umap) |>
  mutate(expr = logcounts(fit)["PLSCR1",]) |>
  sample_frac() |>
  ggplot(aes(x = umap[,1], y = umap[,2])) +
    geom_point(aes(color = expr), size = 0.3) +
    scale_color_viridis_c() +
    facet_wrap(vars(stim)) +
    coord_fixed()

as_tibble(colData(sce)) |>
  mutate(umap = lemur_umap) |>
  mutate(de = assay(fit, "DE")["PLSCR1",]) |>
  sample_frac() |>
  ggplot(aes(x = umap[,1], y = umap[,2])) +
    geom_point(aes(color = de), size = 0.3) +
    scale_color_gradient2() +
    coord_fixed()
```

::: {.callout-note collapse="true"}
## Challenge: How can you use LEMUR to find groups of cells with consistent differential expression?

```{r, paged.print=FALSE}
#| label: find-neighborhood-lemur
#| eval: !expr (! params$skip_execution & ! params$skip_answers)
nei <- lemur::find_de_neighborhoods(fit, group_by = vars(stim, ind), verbose = FALSE)
as_tibble(nei) %>%
  arrange(pval)
```
:::


# Session Info

```{r}
sessionInfo()
```


